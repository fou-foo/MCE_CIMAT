---
title: "SVM"
author: "José Antonio García Ramirez"
output: 
  html_document:
    mathjax: "http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"

---


\begin{equation}
\min_{w,\beta,\epsilon} (1/2)w^tw+Ce^t \epsilon 
\end{equation}
Sujeto a 
\begin{equation}
 YX^t +\beta y+\epsilon \geq e,  \epsilon_i \geq 0
\end{equation}
Donde $C$ es un parámetro de penalización que regularmente debe afinarse. 
La función de costo anterior, después de efectuar los productos punto, tiene un lagrangiano dado por :
\begin{equation}
\begin{split}
L_{p-SVM} = \frac{||w||^2}{2}+C\sum_{i=1}^n\epsilon_i\\- \sum_{i=1}^n \alpha_i[ y_i(x_i^tw +\beta ) -(1-\delta_i)  ]- \sum_{i=1}^n\mu_i \epsilon_i
\end{split}
\end{equation}

Derivando lo anterior con respecto a $w$,$\beta$ y $\epsilon_i$ e igualando a cero tenemos que :
\[
w = \sum_{i=1}^n \alpha_i y_ix_i
\]
\[
0 = \sum_{i=1}^n \alpha_i y_i
\]
\[
\alpha_i = C- \mu_i, \forall i
\]
Por las restricciones de positividad $\alpha_i,\mu_i,\epsilon_i \geq 0$ y sustituyendo lo anterior en (2) tenemos el problema dual:
\begin{equation}
L_{D-SVM} =\sum_{i=1}^n \alpha_i - (1/2)\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jx_i^tx_j 
\end{equation}
Lo anterior proporciona una cota inferior de (2), si maximizamos $L_{D-SVM}$ sujeto a $0\leq \alpha_i \leq C$ y $\sum_{i=i}^n\alpha_iy_i = 0$ y utilizando las derivadas igualadas a cero, las  condiciones de Karush-Khun-Ticher incluyen:
\[
\alpha_i[y_i(x_i^tw+\beta)-(1-\epsilon_i)] = 0
\]
\[
\mu_i\epsilon_i = 0
\]
\[
y_i(x_i^tw+\beta)-(1-\epsilon_i) \geq  0
\]

Lo cual garantiza la unanicidad de de la solución del dual y del primal. Sin embargo el dual es más fácil de optimizar