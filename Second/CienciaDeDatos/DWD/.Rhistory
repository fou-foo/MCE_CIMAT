1:10
s <- read.table('/home/fou/Desktop/MCE/First/AnalisisAlgoritmos/numerosTest.txt')
indice <- which.max(rle(s$V1)$lengths)
rle(s$V1)$values[indice]
rle(s$V1)$lengths[indice]
s$lag <- lag(s$V1)
install.packages("dplyr")
library(dplyr)
s %>% mutate(suma = (V1==lag +1))
install.packages("stringr")
library(stringr)
str_count(string, "123456")
string<- paste(as.character(s$V1), sep ="", collapse = "")
library(stringr)
str_count(string, "123456")
?str_count
str_locate(string, "123456")
str_locate_all (string, "123456")
Kernel.normal <- function(x,y,
#alpha = (.05/2)**.5) #kernel gaussiano
alpha = 0.0013984457316839 ) #para el caso de vinos
{
return(exp(-sum((x-y)**2)/(2*alpha**2)))
}
Kernel.poli <- function(x,y,c = 1, d =2) #kernel polinomial
{
return((sum(x*y)+c)**d)
}
Kernel.kmeans.init <- function(data, cols,  f=Kernel.normal )
{
# data (data.frame): data set con las observaciones
# cols (vector numeric): indices de las columnas para el calculo
#de las distancias de 'data'
#f (function): kernel a usar
#se genera un dataframe con los pares de indices de las observaciones
#con la finalidad de agilizar el calculo de la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
indices <- 1:dim(data)[1]
index <- data.frame(x1 = rep(indices,dim(data)[1]),
x2=rep(indices,each=dim(data)[1]))
library(parallel)#para incrementar la velocidad usaremos calculo multicore
#comienza calculo de la matriz kernel entre todos los pares de observaciones
kernel <- mclapply(FUN=function(i, cols){
f(x=data[index[i,1],][cols],
y=data[index[i,2],][cols])
}, X = 1:dim(index)[1], cols,
mc.cores = detectCores()-2 )
Kernel <- unlist(kernel)
MK <- as.matrix(Kernel)
dim(MK) <-  rep(dim(data)[1], 2)
#termina calculo de matriz de kernel
function(data, sigma,t, k )
{
#data (data.frame) con las observaciones a clasificar
#sigma (numeric): shift mencionado en el paper
#t (numeric): numero de iteraciones
#k (numeric): numero de clusters
data$cluster <- sample(1:k,dim(data)[1], replace = TRUE)#asignacion inicial aleatoria
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- mclapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
}
pis<- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
(1:k)[which.min(d)]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1], mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
return(data$cluster) #cluster finales
}
}
set.seed(0)
r <- 1 #radio
n <- 100 #la cuarta parte del numero de puntos que se van a generar
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/10)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/10)
m.a1 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=1)
r <- 4
n <- 100
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/40)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/40)
m.a2 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=2)
m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
label<- kmeans(m.a, centers = 2) #comparamos el desempeÃ±o de kmeans en vista
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .49%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase, label$cluster))))/dim(m.a)[1] #se calcula el accuracy que es de .265, .735,.4975
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a, cols=1:2)
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 20, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
library(kernlab)
kkmeans.m.a <- kkmeans(as.matrix(m.a[,1:2]), centers=2 ,
kernel = "rbfdot",
kpar = list(sigma = (.05/2)**.5),
alg="kkmeans")#mismo kernel y parametro del kernel
sum(diag(as.matrix(table(m.a$clase, kkmeans.m.a@.Data))))/dim(m.a)[1]#accuracy de 1
p2 <- ggplot(m.a, aes(x=x, y=y, color = factor(kkmeans.m.a@.Data)))+
geom_point() +theme_minimal()+theme(legend.position='none') +
ggtitle('Agrupamiento perfecto de kkmeans')+xlab('')+ylab('')
library(ggpubr)
ggarrange(p1,p2,p3, nrow = 3)
setwd('/home/fou/Desktop')
vinos <- read.csv('wine_quality.csv')
set.seed(0)
muestra.vinos <- sample(1:dim(vinos)[1], 650)
vinos <- vinos[muestra.vinos,]
vinos$cluster <- kmeans(vinos[,2:12], centers=10)$cluster
confusion <- as.matrix(table(vinos$quality, vinos$cluster))
sum(diag(confusion))/dim(vinos)[1]#.09
pca.vinos <- princomp(vinos[,2:12], cor = TRUE)
pca.vinos.prin <- pca.vinos$scores[,1:2]
pca.vinos.prin <- as.data.frame(pca.vinos.prin)
p1 <- ggplot(pca.vinos.prin,#visualizamos nuestro primer conjunto de prueba
aes(x=Comp.1, y=Comp.2, color = factor(vinos$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .12%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
t1 <- Sys.time()
Kernel.kmeans.vinos <- Kernel.kmeans.init(vinos, cols=2:12)
t1 <- Sys.time()-t1
t1
labels <- Kernel.kmeans.vinos(data=vinos, sigma = -1, t=20, k=10)
vinos$cluster <- labels
sum(diag(as.matrix(table(vinos$quality,vinos$cluster))))/dim(vinos)[1] #acurracy con shiff.03
ggplot(pca.vinos.prin,#visualizamos nuestro primer conjunto de prueba
aes(x=Comp.1, y=Comp.2, color = factor(vinos$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .12%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
p3 <- ggplot(pca.vinos.prin,#visualizamos nuestro primer conjunto de prueba
aes(x=Comp.1, y=Comp.2, color = factor(vinos$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de Kernel.kmeans (accuracy .16%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
kkmeans.vinos <- kkmeans(x=as.matrix(vinos[,2:12]), centers=10 ,
kernel = "rbfdot",
alg="kkmeans")
sum(diag(as.matrix(table(vinos$quality, kkmeans.vinos@.Data))))/dim(vinos)[1]#.04603 con el sigma que yo doy .1078, .1237 con el default que vale 0.00153861608749537
ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Primeras dos componentes principales')
ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Agrupamiento de kkmeans (accuracy .12%)')
p2 <- ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Agrupamiento de kkmeans (accuracy .12%)')
ggarrange(p1,p2,p3, nrow = 3)
ggarrange(p1,p2,p3, nrow = 1)
ggarrange(p1,p2,p3, ncol= 3)
p2 <- ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Agrupamiento de kkmeans (accuracy .12%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
ggarrange(p1,p2,p3, ncol= 3)
shiny::runApp('MCE_CIMAT/Second/CienciaDeDatos/DWD')
####################################
#####    J. Antonio Garcia #########
#####   jose.ramirez@cimat.mx ######
####################################
setwd('/home/fou/Desktop/MCE_CIMAT/Second/CienciaDeDatos/DWD/')
#########################################
# Construccion de la primer ilustracion #
#########################################
#generamos una muestra de dim 39 mormal multi y la guardamos
n <- 20
d <- 1000
set.seed(123)
I <- diag(rep(1, d))
library(MASS)
pos <- as.data.frame(mvrnorm(n = n, mu = rep(2.2, d), Sigma = I))
neg <- as.data.frame(mvrnorm(n = n, mu = rep(-2.2, d), Sigma = I))
saveRDS(pos, 'pos1000.RDS')
saveRDS(neg, 'neg1000.RDS')
runApp()
