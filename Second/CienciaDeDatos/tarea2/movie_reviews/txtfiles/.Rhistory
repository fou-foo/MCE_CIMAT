?mshapiro.test(errores)
?mshapiro.test(errores)
mshapiro.test(errores)
data(EuStockMarkets)
C <- t(EuStockMarkets[15:29,1:4])
C
mshapiro.test(C)
mshapiro.test(t(errores))
library(MASS)
mvrnorm(n = 1, c(0,0), diag(c(1,2)), tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
x <- mvrnorm(n = 1000, c(0,0), diag(c(1,2)), tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
x
mshapiro.test(x)
mshapiro.test(t(x))
x <- mvrnorm(n = 100, c(0,0), diag(c(1,2)), tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
mshapiro.test(t(x))
library(mvnormtest)
mshapiro.test(t(errores))
20/30
20/90
27/90
.3*4
27/(90+18)
27/(90+18)
library(MASS)
?mvrnorm()
S1 <- diag(c(1,1))
mu <- c(0,0)
S1 <- diag(c(1,1))
S1
mu <- c(0,0)
S2 <- diag(c(2,2))
S2
m.a1<- mvrnorm(100, mu=mu, Sigma=S1)
m.a2<- mvrnorm(100, mu=mu, Sigma=S2)
m.a1
m.a1<- as.data.frame(m.a1)
View(m.a1)
m.a2<- mvrnorm(100, mu=mu, Sigma=S2)
m.a2 <- as.data.frame(m.a2)
colnames(m.a2) <- colnames(m.a1) <- c('x', 'y')
m.a1$clase <- 1
m.a2$clase <- 2
m.a <- rbind(m.a1, m.a2)
plot(m.a)
plot(m.a[1,], m.a[2,])
plot(m.a[1,], m.a[2,])
m.a[1,]
plot(m.a[,1], m.a[,2])
plot(m.a[,1], m.a[,2])
ggplot(m.a, aes(x=x, y=y, color = clase))+geom_poin()
library(ggplot2)
ggplot(m.a, aes(x=x, y=y, color = clase))+geom_poin()
ggplot(m.a, aes(x=x, y=y, color = clase))+geom_point()
ggplot(m.a, aes(x=x, y=y, color = factor(clase)))+geom_point()
x <- seq(-1, 1, .01)
c <- 1
x <- seq(-c, c, .01)
y <- sqrt(c**2-x**2)
y <- sqrt(c**2-x**2) + rnom(0,0,c/10)
y <- sqrt(c**2-x**2) + rnorm(0,0,c/10)
y1 <- sqrt(c**2-x**2) + rnorm(0,0,c/10)
y2 <- -sqrt(c**2-x**2) - rnorm(0,0,c/10)
rep(x, 2)
x
c <- 1
x <- seq(-c, c, .01)
x
rep(x, 2)
m.a1 <- data.frame(x=rep(x, 2), y = c(y1,y2))
n <- 100
x <- seq(-c, c, length=n)
n <- 100
x <- seq(-c, c, length=n)
y1 <- sqrt(c**2-x**2) + rnorm(n,0,c/10)
y2 <- -sqrt(c**2-x**2) - rnorm(n,0,c/10)
m.a1 <- data.frame(x=rep(x, 2), y = c(y1,y2))
plot(m.a1$x, m.a1$y)
plot(m.a1$x, m.a1$y)
m.a1 <- data.frame(x=rep(x, 2), y = c(y1,y2), clase=1)
plot(m.a1$x, m.a1$y)
plot(m.a1$x, m.a1$y, col='red')
plot(m.a1$x, m.a1$y, col='red', pch = 20)
c <- 4
n <- 100
x <- seq(-c, c, length=n)
y1 <- sqrt(c**2-x**2) + rnorm(n,0,c/10)
y2 <- -sqrt(c**2-x**2) - rnorm(n,0,c/10)
m.a2 <- data.frame(x=rep(x, 2), y = c(y1,y2))
m.a2 <- data.frame(x=rep(x, 2), y = c(y1,y2), clase=2)
m.a2
View(m.a2)
plot(m.a2$x, m.a2$y, col='red', pch = 20)
plot(m.a2$x, m.a2$y, col='blue', pch = 20)
m.a <- rbind(m.a1, m.a2)
library(ggplot2)
ggplot(m.a, aes(x=x, y=y, color = factor(clase)))+geom_point()
kmeans(m.a, centers = 2)
label<- kmeans(m.a, centers = 2)
label
label$cluster
plot(m.a$x, m.a$y, col=label$cluster , pch = 20)
label<- kmeans(m.a, centers = 2)
plot(m.a$x, m.a$y, col=label$cluster , pch = 20)
label<- kmeans(m.a, centers = 2)
plot(m.a$x, m.a$y, col=label$cluster , pch = 20)
label<- kmeans(m.a, centers = 2)
plot(m.a$x, m.a$y, col=label$cluster , pch = 20)
label<- kmeans(m.a, centers = 2)
plot(m.a$x, m.a$y, col=label$cluster , pch = 20)
(.05/2)**.5
dir()
getwd()
directorio <-'C:\\Users\\fou-f\\Desktop\\MCE\\Second\\CienciaDeDatos\\tarea2\\movie_reviews\\txtfiles'
directorio <-'C:\\Users\\fou-f\\Desktop\\MCE\\Second\\CienciaDeDatos\\tarea2\\movie_reviews\\txtfiles'
setwd(directorio)
dir()
#primero veamos las palabras de las opiniones negativas
#para contrastarlas con las palabras de las opiniones positivas y lograr
#realzar el contraste entre ambos grupos quitando las palabras que aparecen en
#ambas bolsas de palabras
preprocesamiento <- function(x)
{
#esta funcion tiene como finalidad extraer todas las palabras
#regresa un dataset con las frecuencias acumuladas por palabras en el corpus
# y una matriz de treminos-documentos
# x (path): path en donde se encuentran lojados en disco duro los documentos
library(tm)
negativos <- Corpus(DirSource(x,recursive=TRUE),
readerControl=list(language="en_US"))
#preprosamiento
negativos <- tm_map(negativos,stripWhitespace)
negativos <- tm_map(negativos,removeNumbers) #tal vez los numero sean inportantes
negativos <- tm_map(negativos,content_transformer(tolower))
negativos <- tm_map(negativos,removePunctuation) #talvez los emoticones tambien sean importantes
negativos <- tm_map(negativos,removeWords,stopwords("english"))
negativos <- tm_map(negativos,stemDocument)
## obtiene matriz de terminos
matriz.neg <- TermDocumentMatrix(negativos)#,control=list(minDocFreq=100))
m.neg <- as.matrix(matriz.neg)
v <- sort(rowSums(m.neg),decreasing=TRUE)
d.neg <- data.frame(word = names(v),freq=v)
d.neg <- d.neg[order(d.neg$word),]
return(list(frecuencias= d.neg, mtd=m.neg ))
}
#######################
negativos <- preprocesamiento(dir()[1]) #preprocesamos los documentos marcados como negativos
positivos <- preprocesamiento(dir()[2]) #preproseamiento de los documentos marcados como positivos
# identificamos las palabras comunes a ambos conjuntos
comunes <- merge(negativos[['frecuencias']],positivos[['frecuencias']]  , by ='word')
#checamos la correlacion de las frecuencias de las palabras en ambos conjuntos
#primero visualmente
library(ggplot2)
ggplot(comunes, aes(x = freq.x, y = freq.y, alpha=.0001 ))+
geom_point()+theme_minimal()+ theme(legend.position="none")+
xlab('Conjunto de palabras en la bolsa "negativas"')+
ylab('Conjunto de palabras en la bolsa "positivas"')+
ggtitle('Frecuencia de palabas comunes en ambos conjuntos de reseñas ')
#parece indicar correlación positiva
#hacemos un test de significancia
cor.test(comunes$freq.x, comunes$freq.y, method = 'pearson', alternative='two.sided')
#como el p-value es casi cero rechazamos la hipotesis nula de no correlacion
#asumimos quue la frecuencia de las palabras es aproximadamente la misma
#por lo que las descartamos del analisis pues solo incrementarian la dimención de la tarea y no
#ayudan a diferenciar el sentimiento
#################################################
todos <- preprocesamiento(dir()) #todos los datos
utiles <- todos[['mtd']]
#primer filtro quitamos las palabras que no aportan información
utiles <- utiles[!(row.names(utiles) %in% as.character(comunes$word)),   ]
df.m <- as.data.frame(t(utiles))
d <- todos[['frecuencias']]
d <- d[!(d$word %in% as.character(comunes$word)),]
#procedemos a eliminar las palabras con menor frecuencia
poquitas <- d$word[d$freq>5] #determinamos las palabras comunes y con una frecuencia mayor a 6
#library(wordcloud)
wordcloud(comunes$word, comunes$freq.x)
utiles <- utiles[(row.names(utiles) %in% as.character(poquitas)),   ]
library(dplyr) #fiiltramode ugual manera la tabla de frecuuencias
df.m <- as.data.frame(t(utiles))
v <- sort(rowSums((utiles)),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#nos quedamos con 443 palabras unicamente
##########seccion un usar PCA para distinguir las recomendaciones postivas de las negativas
#PCA sobre matriz de terminos
R <- cor(df.m)
det(R)
pca <- eigen(R)
cumsum(pca$values)[1:20]/sum(pca$values) #las dos primeras componentes solo explican el 2.8% de la varianza total
rotacion <- pca$vectors
datos.rotados <- as.matrix(df.m)%*%rotacion #rotamos los datos
#graficamos las reseñas en el espacio de componentes principales
datos.rotados <- as.data.frame(datos.rotados)
datos.rotados$sentimiento <- 'negativo'
datos.rotados$sentimiento[501:1000] <- 'Positivo'
p1 <- ggplot(subset(datos.rotados, sentimiento=='negativo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas negativas proyectadas en las dos primeras componentes principales')
p2 <- ggplot(subset(datos.rotados, sentimiento=='Positivo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas positivas proyectadas en las dos primeras componentes principales')
p3 <- ggplot(datos.rotados, aes(x = V1, y = V2, color=sentimiento, alpha=.0001 ))+
geom_point()+ theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Todas las reseñas proyectadas en las dos primeras componentes principales') +
ylim(c(-1/10,1/10)) +xlim(c(-1/10,1/10))
library(ggpubr)
ggarrange(p1, p2, p3, ncol = 1, nrow = 3)
datos.rotados <- datos.rotados%>%
mutate(etiquetaNegativo = !( (V1>=0) & (V2<=0)))
table(datos.rotados$sentimiento, datos.rotados$etiquetaNegativo)
# d.neg <- data.frame(word = names(v),freq=v)
# matriz.pos <- positivos[['mtd']]
# matriz.pos <- as.data.frame(t(matriz.pos))
# matriz.pos$grupo <- factor(rep(1:(dim(matriz.pos)[1]/k), each=k))
# test2 <- matriz.pos%>%group_by(grupo)%>%summarise_all(funs(sum))
# v <- apply(test2[,-1], 2, sum)
# d.pos <- data.frame(word = names(v),freq=v)
# #por fin obtenemos las palabra comunes
# comunes <- merge(d.pos, d.neg  , by ='word') #las mismas 9477 palabras
#################################################
todos <- preprocesamiento(dir()) #todos los datos
utiles <- todos[['mtd']]
dim(utiles)
#primer filtro quitamos las palabras que no aportan información
utiles <- t(utiles)
k <- 5
utiles <- as.data.frame(utiles)
utiles$grupo <- factor(rep(1:(dim(utiles)[1]/k), each=k))
dim(utiles)
test <- utiles %>% group_by(grupo) %>% summarise_all(funs(sum))
dim(test)
v <- apply(test[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
utiles <- utiles[!(colnames(utiles) %in% as.character(comunes$word)),   ]
View(pca)
dim(utiles)
dim(test)
test <- utiles %>% group_by(grupo) %>% summarise_all(funs(sum))
# d.neg <- data.frame(word = names(v),freq=v)
# matriz.pos <- positivos[['mtd']]
# matriz.pos <- as.data.frame(t(matriz.pos))
# matriz.pos$grupo <- factor(rep(1:(dim(matriz.pos)[1]/k), each=k))
# test2 <- matriz.pos%>%group_by(grupo)%>%summarise_all(funs(sum))
# v <- apply(test2[,-1], 2, sum)
# d.pos <- data.frame(word = names(v),freq=v)
# #por fin obtenemos las palabra comunes
# comunes <- merge(d.pos, d.neg  , by ='word') #las mismas 9477 palabras
#################################################
todos <- preprocesamiento(dir()) #todos los datos
utiles <- todos[['mtd']]
#primer filtro quitamos las palabras que no aportan información
utiles <- t(utiles)
k <- 5
utiles <- as.data.frame(utiles)
utiles$grupo <- factor(rep(1:(dim(utiles)[1]/k), each=k))
test <- utiles %>% group_by(grupo) %>% summarise_all(funs(sum))
dim(test)
dir()
dim(test)
head(test[,1:5])
v <- apply(test[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
colnames(test)
test <- test[!(colnames(test) %in% as.character(comunes$word)),   ]
test <- utiles %>% group_by(grupo) %>% summarise_all(funs(sum))
dim(test)
head(test[,1:5])
v <- apply(test[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
v <- apply(test[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
22261-9477
utiles.fino <- test[, !(colnames(test) %in% as.character(comunes$word))   ]
head(utiles.fino[1,1:5])
d$word[d$freq>5]
#procedemos a eliminar las palabras con menor frecuencia
poquitas <- d$word[d$freq>5] #determinamos las palabras comunes y con una frecuencia mayor a 6
#procedemos a eliminar las palabras con menor frecuencia
poquitas <- d$word[d$freq>5] #determinamos las palabras comunes y con una frecuencia mayor a 6
utiles.fino <- utiles.fino[(colnames(utiles.fino) %in% as.character(poquitas)),   ]
utiles.fino <- test[, !(colnames(test) %in% as.character(comunes$word))   ]
#procedemos a eliminar las palabras con menor frecuencia
poquitas <- d$word[d$freq>5] #determinamos las palabras comunes y con una frecuencia mayor a 6
v <- apply(test[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
utiles.fino <- test[, !(colnames(test) %in% as.character(comunes$word))   ]
#procedemos a eliminar las palabras con menor frecuencia
poquitas <- d$word[d$freq>5] #determinamos las palabras comunes y con una frecuencia mayor a 6
utiles.fino <- utiles.fino[, (colnames(utiles.fino) %in% as.character(poquitas))   ]
df.m <- as.data.frame(utiles.fino)
v <- apply(utiles.fino[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
#nos quedamos con 442 palabras unicamente
##########seccion un usar PCA para distinguir las recomendaciones postivas de las negativas
#PCA sobre matriz de terminos
R <- cor(df.m)
det(R)
dim(R)
pca <- eigen(R)
cumsum(pca$values)[1:20]/sum(pca$values) #las dos primeras componentes solo explican el 2.8% de la varianza total
cumsum(pca$values)[1:20]/sum(pca$values) #las dos primeras componentes solo explican el 4.7% de la varianza total
rotacion <- pca$vectors
datos.rotados <- as.matrix(df.m)%*%rotacion #rotamos los datos
#graficamos las reseñas en el espacio de componentes principales
datos.rotados <- as.data.frame(datos.rotados)
datos.rotados$sentimiento <- 'negativo'
dim(datos.rotados)
datos.rotados$sentimiento <- 'negativo'
datos.rotados$sentimiento[101:200] <- 'Positivo'
p1 <- ggplot(subset(datos.rotados, sentimiento=='negativo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas negativas proyectadas en las dos primeras componentes principales')
p2 <- ggplot(subset(datos.rotados, sentimiento=='Positivo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas positivas proyectadas en las dos primeras componentes principales')
p3 <- ggplot(datos.rotados, aes(x = V1, y = V2, color=sentimiento, alpha=.0001 ))+
geom_point()+ theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Todas las reseñas proyectadas en las dos primeras componentes principales') +
ylim(c(-1/10,1/10)) +xlim(c(-1/10,1/10))
library(ggpubr)
ggarrange(p1, p2, p3, ncol = 1, nrow = 3)
p1
p2
p3
library(ggpubr)
p3 <- ggplot(datos.rotados, aes(x = V1, y = V2, color=sentimiento, alpha=.0001 ))+
geom_point()+ theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Todas las reseñas proyectadas en las dos primeras componentes principales')
p3
p1 <- ggplot(subset(datos.rotados, sentimiento=='negativo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas negativas proyectadas en las dos primeras componentes principales')
p2 <- ggplot(subset(datos.rotados, sentimiento=='Positivo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas positivas proyectadas en las dos primeras componentes principales')
p3 <- ggplot(datos.rotados, aes(x = V1, y = V2, color=sentimiento, alpha=.0001 ))+
geom_point()+ theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Todas las reseñas proyectadas en las dos primeras componentes principales')
p1
p2
p3
p1
datos.rotados <- datos.rotados%>%
mutate(etiquetaNegativo = !( (V1<=0) & (V2<=0)))
table(datos.rotados$sentimiento, datos.rotados$etiquetaNegativo)
4/200
# d.neg <- data.frame(word = names(v),freq=v)
# matriz.pos <- positivos[['mtd']]
# matriz.pos <- as.data.frame(t(matriz.pos))
# matriz.pos$grupo <- factor(rep(1:(dim(matriz.pos)[1]/k), each=k))
# test2 <- matriz.pos%>%group_by(grupo)%>%summarise_all(funs(sum))
# v <- apply(test2[,-1], 2, sum)
# d.pos <- data.frame(word = names(v),freq=v)
# #por fin obtenemos las palabra comunes
# comunes <- merge(d.pos, d.neg  , by ='word') #las mismas 9477 palabras
#################################################
todos <- preprocesamiento(dir()) #todos los datos
utiles <- todos[['mtd']]
#primer filtro quitamos las palabras que no aportan información
utiles <- t(utiles)
k <- 2 #numero de agrupacion
utiles <- as.data.frame(utiles)
factor(rep(1:(dim(utiles)[1]/k), each=k))
utiles$grupo <- factor(rep(1:(dim(utiles)[1]/k), each=k))
test <- utiles %>% group_by(grupo) %>% summarise_all(funs(sum))
dim(test)
head(test[,1:5])
v <- apply(test[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
utiles.fino <- test[, !(colnames(test) %in% as.character(comunes$word))   ]
#procedemos a eliminar las palabras con menor frecuencia
poquitas <- d$word[d$freq>5] #determinamos las palabras comunes y con una frecuencia mayor a 6
utiles.fino <- utiles.fino[, (colnames(utiles.fino) %in% as.character(poquitas))   ]
df.m <- as.data.frame(utiles.fino)
v <- apply(utiles.fino[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
#nos quedamos con 442 palabras unicamente
##########seccion un usar PCA para distinguir las recomendaciones postivas de las negativas
#PCA sobre matriz de terminos
R <- cor(df.m)
det(R)
pca <- eigen(R)
cumsum(pca$values)[1:20]/sum(pca$values) #las dos primeras componentes solo explican el 4.7% de la varianza total
rotacion <- pca$vectors
datos.rotados <- as.matrix(df.m)%*%rotacion #rotamos los datos
#graficamos las reseñas en el espacio de componentes principales
datos.rotados <- as.data.frame(datos.rotados)
datos.rotados$sentimiento <- 'negativo'
datos.rotados$sentimiento[251:500] <- 'Positivo'
p1 <- ggplot(subset(datos.rotados, sentimiento=='negativo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas negativas proyectadas en las dos primeras componentes principales')
p2 <- ggplot(subset(datos.rotados, sentimiento=='Positivo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas positivas proyectadas en las dos primeras componentes principales')
p3 <- ggplot(datos.rotados, aes(x = V1, y = V2, color=sentimiento, alpha=.0001 ))+
geom_point()+ theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Todas las reseñas proyectadas en las dos primeras componentes principales')
ggarrange(p1, p2, p3, ncol = 1, nrow = 3)
datos.rotados <- datos.rotados%>%
mutate(etiquetaNegativo = !( (V1<=0) & (V2<=0)))
table(datos.rotados$sentimiento, datos.rotados$etiquetaNegativo)
p1
p2
p3
datos.rotados <- datos.rotados%>%
mutate(etiquetaNegativo = !( (V1>=0) & (V2>=0)))
table(datos.rotados$sentimiento, datos.rotados$etiquetaNegativo)
114/500
# d.neg <- data.frame(word = names(v),freq=v)
# matriz.pos <- positivos[['mtd']]
# matriz.pos <- as.data.frame(t(matriz.pos))
# matriz.pos$grupo <- factor(rep(1:(dim(matriz.pos)[1]/k), each=k))
# test2 <- matriz.pos%>%group_by(grupo)%>%summarise_all(funs(sum))
# v <- apply(test2[,-1], 2, sum)
# d.pos <- data.frame(word = names(v),freq=v)
# #por fin obtenemos las palabra comunes
# comunes <- merge(d.pos, d.neg  , by ='word') #las mismas 9477 palabras
#################################################
todos <- preprocesamiento(dir()) #todos los datos
utiles <- todos[['mtd']]
#primer filtro quitamos las palabras que no aportan información
utiles <- t(utiles)
k <- 5 #numero de agrupacion
utiles <- as.data.frame(utiles)
utiles$grupo <- factor(rep(1:(dim(utiles)[1]/k), each=k))
test <- utiles %>% group_by(grupo) %>% summarise_all(funs(sum))
dim(test)
head(test[,1:5])
v <- apply(test[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
utiles.fino <- test[, !(colnames(test) %in% as.character(comunes$word))   ]
#procedemos a eliminar las palabras con menor frecuencia
poquitas <- d$word[d$freq>5] #determinamos las palabras comunes y con una frecuencia mayor a 6
utiles.fino <- utiles.fino[, (colnames(utiles.fino) %in% as.character(poquitas))   ]
df.m <- as.data.frame(utiles.fino)
v <- apply(utiles.fino[,-1], 2, sum)
d <- data.frame(word = names(v),freq=v)
#nos quedamos con 442 palabras unicamente
##########seccion un usar PCA para distinguir las recomendaciones postivas de las negativas
#PCA sobre matriz de terminos
R <- cor(df.m)
det(R)
pca <- eigen(R)
cumsum(pca$values)[1:20]/sum(pca$values) #las dos primeras componentes solo explican el 4.7% de la varianza total
rotacion <- pca$vectors
datos.rotados <- as.matrix(df.m)%*%rotacion #rotamos los datos
#graficamos las reseñas en el espacio de componentes principales
datos.rotados <- as.data.frame(datos.rotados)
datos.rotados$sentimiento <- 'negativo'
datos.rotados$sentimiento[251:500] <- 'Positivo'
datos.rotados$sentimiento <- 'negativo'
datos.rotados$sentimiento <- 'negativo'
datos.rotados$sentimiento[101:200] <- 'Positivo'
p1 <- ggplot(subset(datos.rotados, sentimiento=='negativo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas negativas proyectadas en las dos primeras componentes principales')
p2 <- ggplot(subset(datos.rotados, sentimiento=='Positivo'),
aes(x = V1, y = V2, color=sentimiento, alpha=.01 ))+
geom_point()+theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Reseñas positivas proyectadas en las dos primeras componentes principales')
p3 <- ggplot(datos.rotados, aes(x = V1, y = V2, color=sentimiento, alpha=.0001 ))+
geom_point()+ theme_minimal()+
xlab('Primer componente principal')+
ylab('Segundo componente principal')+
ggtitle('Todas las reseñas proyectadas en las dos primeras componentes principales')
p1
datos.rotados <- datos.rotados%>%
mutate(etiquetaNegativo = !( (V1<=0) & (V2<=0)))
table(datos.rotados$sentimiento, datos.rotados$etiquetaNegativo)
ggarrange(p1, p2, p3, ncol = 1, nrow = 3)
cumsum(pca$values)[1:20]/sum(pca$values) #las dos primeras componentes solo explican el 4.7% de la varianza total
4/200
