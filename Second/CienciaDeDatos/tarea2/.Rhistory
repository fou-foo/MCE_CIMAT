exponential.kernel <- stringdot(type='exponential',
lambda = .5, normalized = TRUE)
k <- spectrum.kernel
#construimos la primer matriz de similaridades, con todos los datos del archivo
K.Gram <- kernelMatrix(kernel=k ,
x=strings$description_x,
y=strings$description_y)
#realizamos Kernel - pca considerando un kernel gaussiano con parÃ¡metro igual a la
#unidad, porque la matriz esta de similaridades esta centrada
kernel.pca <- kpca(K.Gram, kernel = "rbfdot", kpar = list(sigma = 1))
# revisamos la varianza explicada por las 10 primeras componentes principales
val.pro <- kernel.pca@eig
plot(cumsum(kernel.pca@eig)[1:10]/sum(kernel.pca@eig))
#guardamos los datos rotados: NOTA no tiene porque ser cuadrada la
# matriz de vectores propios, pues depende del rango de la matriz de
# similaridades
rotacion <- kernel.pca@rotated
rotacion <- as.data.frame(rotacion)
rotacion$etiqueta <- factor(strings$same_security)
library(ggplot2) #graficamos la proyeccion en las dos primeras componentes
library(ggplot2) #graficamos la proyeccion en las dos primeras componentes
ggplot(rotacion, aes(x =V1, y = V2, color=etiqueta, alpha=.1))+ geom_point()+
ggtitle('Kernel: spectrum, n=3, lambda=1') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE)
names(strings)
View(strings)
(table(x1.len, x2.len))# 4 es el mayor
table(y1.len, y2.len) #5 es el mayor
set.seed(0) #fijamos la semilla para comparar la pseudo muestra aleatoria
K.Gram.train <- kernelMatrix(kernel=k , #construimos la matriz de similaridades
x=strings$description_x) #considerando solo las etiquetas 'description_x'
kernel.pca.train <- kpca(K.Gram.train, kernel = "rbfdot", kpar = list(sigma = 1)) #kernel pca igual que en el caso anterior
index <- sample(1:dim(strings)[1], round(dim(strings)[1]/1) ) #indice de la muestra aleatoria
componentes.prin.train <- kernel.pca.train@pcv #vectores propios de kernel-pca
test <- strings[index, ] #conjunto de test
K.Gram.test <- kernelMatrix(kernel=k ,y = strings$description_x,
x=test$description_y) #calculamos la similaridad entre el
#conjunto de train y el de test
res <- predict(kernel.pca.train, K.Gram.test) #rotamos el conjunto test
#se procede a calculo explicito de las distancias de cada observacion
#en el conjunto test contra todas las observaciones rotadas en el conjunto de train
foo <- matrix(rep(-1, dim(res)[1]*dim(strings)[1]), ncol = dim(strings)[1])
for (i in 1:dim(foo)[1])
{
for(j in 1:dim(foo)[2])
{
foo[i, j] <- sum((res[i,]-kernel.pca.train@rotated[j,])**2)**.5
}
}
mas.proximo <- apply(foo, 1, which.min ) #determinamos la observacion mas cercana del conjunto train
test$res <- strings$description_x[mas.proximo] #agregamos al test para comparar
test$resul <- test$description_x == test$res #calificamos prediccion vs etiqueta
table(test$same_security,test$resul)/sum(table(test$same_security,test$resul)) #resumen
table(strings$same_security)/dim(strings)[1]
#spectrum 1 lambda =1: .224 .2009
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 4 lambda =1: .219 .2289
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 4 lambda =1: .219 .2289
#boundrange 1 : .2242 .2009
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 4 lambda =1: .219 .2289
#boundrange 1 : .2242 .2009
#boundrange 2 : .219 .214
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 4 lambda =1: .219 .2289
#boundrange 1 : .2242 .2009
#boundrange 2 : .219 .214
#boundrange 3 : .219 .219
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 4 lambda =1: .219 .2289
#boundrange 1 : .2242 .2009
#boundrange 2 : .219 .214
#boundrange 3 : .219 .219
#boundrange 5 : .2242 .228
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 4 lambda =1: .219 .2289
#boundrange 1 : .2242 .2009
#boundrange 2 : .219 .214
#boundrange 3 : .219 .219
#boundrange 5 : .2242 .228
#boundrange 10 : .2242 .2336
#spectrum 1 lambda =1: .224 .2009
#spectrum 2 lambda =1: .224 .2289
#spectrum 3 lambda =1: .219 .2336
#spectrum 4 lambda =1: .219 .2289
#boundrange 1 : .2242 .2009
#boundrange 2 : .219 .214
#boundrange 3 : .219 .219
#boundrange 5 : .2242 .228
#boundrange 10 : .2242 .2336
#constant : .2336 .112
table(test$same_security,test$resul)/sum(table(test$same_security,test$resul)) #resumen
View(test)
set.seed(0) #fijamos la semilla para comparar la pseudo muestra aleatoria
K.Gram.train <- kernelMatrix(kernel=k , #construimos la matriz de similaridades
x=strings$description_x) #considerando solo las etiquetas 'description_x'
kernel.pca.train <- kpca(K.Gram.train, kernel = "rbfdot", kpar = list(sigma = 1)) #kernel pca igual que en el caso anterior
index <- sample(1:dim(strings)[1], round(dim(strings)[1]/10) ) #indice de la muestra aleatoria
componentes.prin.train <- kernel.pca.train@pcv #vectores propios de kernel-pca
test <- strings[index, ] #conjunto de test
K.Gram.test <- kernelMatrix(kernel=k ,y = strings$description_x,
x=test$description_y) #calculamos la similaridad entre el
#conjunto de train y el de test
res <- predict(kernel.pca.train, K.Gram.test) #rotamos el conjunto test
#se procede a calculo explicito de las distancias de cada observacion
#en el conjunto test contra todas las observaciones rotadas en el conjunto de train
foo <- matrix(rep(-1, dim(res)[1]*dim(strings)[1]), ncol = dim(strings)[1])
###########inciso b
set.seed(0) #fijamos la semilla para comparar la pseudo muestra aleatoria
K.Gram.train <- kernelMatrix(kernel=k , #construimos la matriz de similaridades
x=strings$description_x) #considerando solo las etiquetas 'description_x'
kernel.pca.train <- kpca(K.Gram.train, kernel = "rbfdot", kpar = list(sigma = 1)) #kernel pca igual que en el caso anterior
index <- sample(1:dim(strings)[1], round(dim(strings)[1]/10) ) #indice de la muestra aleatoria
componentes.prin.train <- kernel.pca.train@pcv #vectores propios de kernel-pca
test <- strings[index, ] #conjunto de test
K.Gram.test <- kernelMatrix(kernel=k ,y = strings$description_x,
x=test$description_y) #calculamos la similaridad entre el
#conjunto de train y el de test
res <- predict(kernel.pca.train, K.Gram.test) #rotamos el conjunto test
#se procede a calculo explicito de las distancias de cada observacion
#en el conjunto test contra todas las observaciones rotadas en el conjunto de train
foo <- matrix(rep(-1, dim(res)[1]*dim(strings)[1]), ncol = dim(strings)[1])
for (i in 1:dim(foo)[1])
{
for(j in 1:dim(foo)[2])
{
foo[i, j] <- sum((res[i,]-kernel.pca.train@rotated[j,])**2)**.5
}
}
mas.proximo <- apply(foo, 1, which.min ) #determinamos la observacion mas cercana del conjunto train
train <- kernel.pca.train@rotated
str(train)
train <- kernel.pca.train@rotated[,1:2]
train <- as.data.frame(train)
View(train)
ggplot(train, aes(x =V1, y = V2, color=I('navy'), alpha=.1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE)
test$res <- strings$description_x[mas.proximo] #agregamos al test para comparar
test$resul <- test$description_x == test$res #calificamos prediccion vs etiqueta
table(test$same_security,test$resul)/sum(table(test$same_security,test$resul)) #resumen
table(strings$same_security)/dim(strings)[1]
str(res)
res <- as.data.frame(res)
View(res)
res <- as.data.frame(res)
ggplot(train, aes(x =V1, y = V2, color=I('navy'), alpha=.1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(res[, 1:2], aes(x =V1, y = V2, color=I('orange')))
class(res)
class(res)
class(train)
ggplot(train, aes(x =V1, y = V2, color=I('navy'), alpha=.1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE)
ggplot(train, aes(x =V1, y = V2, color=I('navy'), alpha=.05))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE)
ggplot(train, aes(x =V1, y = V2, color=I('navy'), alpha=.05))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2, color=I('orange')))
str(mas.proximo)
y_hat <- train[mas.proximo,]
str(y_hat)
ggplot(train, aes(x =V1, y = V2, color=I('navy'), alpha=.05))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2, color=I('orange')))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2, color=I('green4')))
###########inciso b
set.seed(0) #fijamos la semilla para comparar la pseudo muestra aleatoria
K.Gram.train <- kernelMatrix(kernel=k , #construimos la matriz de similaridades
x=strings$description_x) #considerando solo las etiquetas 'description_x'
kernel.pca.train <- kpca(K.Gram.train, kernel = "rbfdot", kpar = list(sigma = 1)) #kernel pca igual que en el caso anterior
index <- sample(1:dim(strings)[1], round(dim(strings)[1]/100) ) #indice de la muestra aleatoria
componentes.prin.train <- kernel.pca.train@pcv #vectores propios de kernel-pca
test <- strings[index, ] #conjunto de test
K.Gram.test <- kernelMatrix(kernel=k ,y = strings$description_x,
x=test$description_y) #calculamos la similaridad entre el
#conjunto de train y el de test
res <- predict(kernel.pca.train, K.Gram.test) #rotamos el conjunto test
#se procede a calculo explicito de las distancias de cada observacion
#en el conjunto test contra todas las observaciones rotadas en el conjunto de train
foo <- matrix(rep(-1, dim(res)[1]*dim(strings)[1]), ncol = dim(strings)[1])
for (i in 1:dim(foo)[1])
{
for(j in 1:dim(foo)[2])
{
foo[i, j] <- sum((res[i,]-kernel.pca.train@rotated[j,])**2)**.5
}
}
mas.proximo <- apply(foo, 1, which.min ) #determinamos la observacion mas cercana del conjunto train
train <- kernel.pca.train@rotated[,1:2]
train <- as.data.frame(train)
test$res <- strings$description_x[mas.proximo] #agregamos al test para comparar
test$resul <- test$description_x == test$res #calificamos prediccion vs etiqueta
table(test$same_security,test$resul)/sum(table(test$same_security,test$resul)) #resumen
table(strings$same_security)/dim(strings)[1]
res <- as.data.frame(res)
y_hat <- train[mas.proximo,]
ggplot(train, aes(x =V1, y = V2, color=I('navy'), alpha=.05))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2, color=I('orange')))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2, color=I('green4')))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.05))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2, color=I('orange')))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2, color=I('green4')))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.05))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=I('navy'), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=I('red'), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.05))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=I('pink'), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=I('green'), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=I('pink'), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=I('green'), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=I('purple'), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=I('green'), alpha =1))
vis <- cbind(y_hat, res)
vis$index <- 1:dim(vis)[1]
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=vis$index, alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=vis$index, alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))
factor(vis$index)
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.5))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.5))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=.7))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))
letters()
letters
vis$index <- letters[1:dim(vis)[1]]
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=factor(vis$index), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
vis$index <- colors()[1:dim(vis)[1]]
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
vis$index <- colors()[1:dim(vis)[1]+10]
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
colors()
vis$index <- colors()[1:dim(vis)[1]+100]
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
vis$index <- rep(c('purple', 'navy', 'orange'), 7)[1:dim(vis)[1]]
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
colors()
colores <- c('purple', 'navy', 'orange', 'tomato1', 'turquoise', 'magenta1',
'hotpink')
vis$index <- rep(colores, 3)[1:dim(vis)[1]]
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
vis$index
vis$index <- rep(colores, 3)
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
colores <- c('purple', 'navy', 'orange', 'tomato1', 'turquoise', 'magenta1',
'green4')
vis$index <- rep(colores, 3)
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
colores <- c('purple', 'navy', 'orange', 'red', 'turquoise', 'magenta1',
'green4')
vis$index <- rep(colores, 3)
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
colores <- c('purple', 'navy', 'orange', 'red', 'turquoise', 'blue4',
'green4')
vis$index <- rep(colores, 3)
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
colores <- c('purple', 'black', 'orange', 'red', 'turquoise', 'blue4',
'green4')
vis$index <- rep(colores, 3)
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
###########inciso b
set.seed(0) #fijamos la semilla para comparar la pseudo muestra aleatoria
K.Gram.train <- kernelMatrix(kernel=k , #construimos la matriz de similaridades
x=strings$description_x) #considerando solo las etiquetas 'description_x'
kernel.pca.train <- kpca(K.Gram.train, kernel = "rbfdot", kpar = list(sigma = 1)) #kernel pca igual que en el caso anterior
index <- sample(1:dim(strings)[1], round(24) ) #indice de la muestra aleatoria
componentes.prin.train <- kernel.pca.train@pcv #vectores propios de kernel-pca
test <- strings[index, ] #conjunto de test
K.Gram.test <- kernelMatrix(kernel=k ,y = strings$description_x,
x=test$description_y) #calculamos la similaridad entre el
#conjunto de train y el de test
res <- predict(kernel.pca.train, K.Gram.test) #rotamos el conjunto test
#se procede a calculo explicito de las distancias de cada observacion
#en el conjunto test contra todas las observaciones rotadas en el conjunto de train
foo <- matrix(rep(-1, dim(res)[1]*dim(strings)[1]), ncol = dim(strings)[1])
for (i in 1:dim(foo)[1])
{
for(j in 1:dim(foo)[2])
{
foo[i, j] <- sum((res[i,]-kernel.pca.train@rotated[j,])**2)**.5
}
}
mas.proximo <- apply(foo, 1, which.min ) #determinamos la observacion mas cercana del conjunto train
train <- kernel.pca.train@rotated[,1:2]
train <- as.data.frame(train)
test$res <- strings$description_x[mas.proximo] #agregamos al test para comparar
test$resul <- test$description_x == test$res #calificamos prediccion vs etiqueta
table(test$same_security,test$resul)/sum(table(test$same_security,test$resul)) #resumen
table(strings$same_security)/dim(strings)[1]
res <- as.data.frame(res)
y_hat <- train[mas.proximo,]
vis <- cbind(y_hat, res)
colores <- c('purple', 'black', 'orange', 'red', 'turquoise', 'blue4',
'green4', 'salmon')
vis$index <- rep(colores, 3)
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
xlim(c(-30, 15))+ylim(c(-20,15))
ggplot(train, aes(x =V1, y = V2, color=I('lightblue'), alpha=1))+ geom_point()+
ggtitle('Train') + theme_minimal() + xlab('Primer kernel-componente principal')+
ylab('Segunda kernel-componente principal')+ guides(alpha = FALSE) +
geom_point(data=res[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
geom_point(data=y_hat[, 1:2], aes(x =V1, y = V2,
color=(vis$index), alpha =1))+
xlim(c(-26, 13))+ylim(c(-15,15))
###########inciso b
set.seed(0) #fijamos la semilla para comparar la pseudo muestra aleatoria
K.Gram.train <- kernelMatrix(kernel=k , #construimos la matriz de similaridades
x=strings$description_x) #considerando solo las etiquetas 'description_x'
kernel.pca.train <- kpca(K.Gram.train, kernel = "rbfdot", kpar = list(sigma = 1)) #kernel pca igual que en el caso anterior
index <- sample(1:dim(strings)[1], dim(strings)[1]  ) #indice de la muestra aleatoria
?sample
max(table(index)
)
componentes.prin.train <- kernel.pca.train@pcv #vectores propios de kernel-pca
###########inciso b
set.seed(0) #fijamos la semilla para comparar la pseudo muestra aleatoria
K.Gram.train <- kernelMatrix(kernel=k , #construimos la matriz de similaridades
x=strings$description_x) #considerando solo las etiquetas 'description_x'
kernel.pca.train <- kpca(K.Gram.train, kernel = "rbfdot", kpar = list(sigma = 1)) #kernel pca igual que en el caso anterior
index <- sample(1:dim(strings)[1], dim(strings)[1]  ) #indice de la muestra aleatoria
componentes.prin.train <- kernel.pca.train@pcv #vectores propios de kernel-pca
test <- strings[index, ] #conjunto de test
K.Gram.test <- kernelMatrix(kernel=k ,y = strings$description_x,
x=test$description_y) #calculamos la similaridad entre el
#conjunto de train y el de test
res <- predict(kernel.pca.train, K.Gram.test) #rotamos el conjunto test
#se procede a calculo explicito de las distancias de cada observacion
#en el conjunto test contra todas las observaciones rotadas en el conjunto de train
foo <- matrix(rep(-1, dim(res)[1]*dim(strings)[1]), ncol = dim(strings)[1])
for (i in 1:dim(foo)[1])
{
for(j in 1:dim(foo)[2])
{
foo[i, j] <- sum((res[i,]-kernel.pca.train@rotated[j,])**2)**.5
}
}
mas.proximo <- apply(foo, 1, which.min ) #determinamos la observacion mas cercana del conjunto train
train <- kernel.pca.train@rotated[,1:2]
train <- as.data.frame(train)
test$res <- strings$description_x[mas.proximo] #agregamos al test para comparar
test$resul <- test$description_x == test$res #calificamos prediccion vs etiqueta
table(test$same_security,test$resul)/sum(table(test$same_security,test$resul)) #resumen
.2292+.2338
names(strings)
round(table(test$same_security,test$resul)/sum(table(test$same_security,test$resul)),2) #resumen
