#uso sobre
set.seed(0)
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a, cols=1:2)
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 200, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
######### Implementacion de kernel k-means con shift basado en el paper:
#########Inderjit Dhillon, Yuqiang Guan and Brian Kulis.
#####A Unified view of Kernel k-means, Spectral Clustering and Graph Cuts.
Kernel.normal <- function(x,y,
alpha = (.05/2)**.5) #kernel gaussiano
#  alpha = 0.0013984457316839 ) #para el caso de vinos
{
return(exp(-sum((x-y)**2)/(2*alpha**2)))
}
Kernel.poli <- function(x,y,c = 1, d =2) #kernel polinomial
{
return((sum(x*y)+c)**d)
}
Kernel.kmeans.init <- function(data, cols,  f=Kernel.normal )
{
# data (data.frame): data set con las observaciones
# cols (vector numeric): indices de las columnas para el calculo
#de las distancias de 'data'
#f (function): kernel a usar
#se genera un dataframe con los pares de indices de las observaciones
#con la finalidad de agilizar el calculo de la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
n <- dim(data)[1]
indices <- 1:n
#modificacion del 31 de marzo solo calcular la diagonal superior de la matriz K
index <- data.frame(x1 = rep(indices,times=n:1))
x2 <-lapply(FUN = function(x){
return(n:x)
}, 1:n)
x2 <- unlist(x2)
index$x2 <-x2
index <- index[ order(index$x1, index$x2),]
library(parallel)#para incrementar la velocidad usaremos calculo multicore
#comienza calculo de la matriz superior del kernel entre todos los pares de observaciones
kernel.sup <- lapply(FUN=function(i, cols){
f(x=data[index[i,1],][cols],
y=data[index[i,2],][cols])
}, X = 1:dim(index)[1], cols)#,  mc.cores = detectCores()-2 )
kernel.sup <- unlist(kernel.sup)
MK <- matrix(rep(0, n*n), ncol = n)
for(i in 1:dim(index)[1])
{
x <- index[i, 1]
y <- index[i, 2]
MK[x, y] <- kernel.sup[i]
MK[y, x] <- kernel.sup[i]
}
#termina calculo de matriz de kernel
function(data, sigma,t, k )
{
#data (data.frame) con las observaciones a clasificar
#sigma (numeric): shift mencionado en el paper
#t (numeric): numero de iteraciones
#k (numeric): numero de clusters
data$cluster <- sample(1:k,dim(data)[1], replace = TRUE)#asignacion inicial aleatoria
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis<- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
(1:k)[which.min(d)]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
return(data$cluster) #cluster finales
}
}
############################################################
set.seed(0)
#####simulacion de datos parecidos a los del paper mencionado
#####son dos circunferencias con centro (.5,.5) y radios 1 y 4
#####se agrega en cada eje ruido ~ N(0,sigma=1/10 ) y N(0,1/10)
r <- 1 #radio
n <- 100 #la cuarta parte del numero de puntos que se van a generar
#se genera la primer circunferencia con ruido
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/10)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/10)
m.a1 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=1)
#se genera la segunda circunferencia con ruido
r <- 4
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/40)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/40)
m.a2 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=2)
m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
label<- kmeans(m.a, centers = 2) #comparamos el desempeño de kmeans en vista
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .49%)') +xlab('') + ylab('')
#visualizamos los resultados
#de clasificacion usando kmeans
sum(diag(as.matrix(table(m.a$clase, label$cluster))))/dim(m.a)[1] #se calcula el accuracy que es de .265, .735,.4975
p1
set.seed(0)
set.seed(0)
label<- kmeans(m.a, centers = 2, nstart = 100) #comparamos el desempeño de kmeans en vista
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .49%)') +xlab('') + ylab('')
p1
#visualizamos los resultados
#de clasificacion usando kmeans
sum(diag(as.matrix(table(m.a$clase, label$cluster))))/dim(m.a)[1] #se calcula el accuracy que es de .265, .735,.4975
#uso sobre
set.seed(0)
data <- m.a
sigma = -1
t = 20
k =2
cols=1:2
# data (data.frame): data set con las observaciones
# cols (vector numeric): indices de las columnas para el calculo
#de las distancias de 'data'
#f (function): kernel a usar
#se genera un dataframe con los pares de indices de las observaciones
#con la finalidad de agilizar el calculo de la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
n <- dim(data)[1]
indices <- 1:n
#modificacion del 31 de marzo solo calcular la diagonal superior de la matriz K
index <- data.frame(x1 = rep(indices,times=n:1))
x2 <-lapply(FUN = function(x){
return(n:x)
}, 1:n)
x2 <- unlist(x2)
index$x2 <-x2
index <- index[ order(index$x1, index$x2),]
library(parallel)#para incrementar la velocidad usaremos calculo multicore
#comienza calculo de la matriz superior del kernel entre todos los pares de observaciones
kernel.sup <- lapply(FUN=function(i, cols){
f(x=data[index[i,1],][cols],
y=data[index[i,2],][cols])
}, X = 1:dim(index)[1], cols)#,  mc.cores = detectCores()-2 )
f=Kernel.normal
# data (data.frame): data set con las observaciones
# cols (vector numeric): indices de las columnas para el calculo
#de las distancias de 'data'
#f (function): kernel a usar
#se genera un dataframe con los pares de indices de las observaciones
#con la finalidad de agilizar el calculo de la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
n <- dim(data)[1]
indices <- 1:n
#modificacion del 31 de marzo solo calcular la diagonal superior de la matriz K
index <- data.frame(x1 = rep(indices,times=n:1))
x2 <-lapply(FUN = function(x){
return(n:x)
}, 1:n)
x2 <- unlist(x2)
index$x2 <-x2
index <- index[ order(index$x1, index$x2),]
library(parallel)#para incrementar la velocidad usaremos calculo multicore
#comienza calculo de la matriz superior del kernel entre todos los pares de observaciones
kernel.sup <- lapply(FUN=function(i, cols){
f(x=data[index[i,1],][cols],
y=data[index[i,2],][cols])
}, X = 1:dim(index)[1], cols)#,  mc.cores = detectCores()-2 )
kernel.sup <- unlist(kernel.sup)
MK <- matrix(rep(0, n*n), ncol = n)
for(i in 1:dim(index)[1])
{
x <- index[i, 1]
y <- index[i, 2]
MK[x, y] <- kernel.sup[i]
MK[y, x] <- kernel.sup[i]
}
#data (data.frame) con las observaciones a clasificar
#sigma (numeric): shift mencionado en el paper
#t (numeric): numero de iteraciones
#k (numeric): numero de clusters
data$cluster <- sample(1:k,dim(data)[1], replace = TRUE)#asignacion inicial aleatoria
data$cluster
x<-1
i<-1
#parte del calculo que no depende del shift
d <- rep(-1, k)
d
z <-1
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
is.na(d)
d[is.na(d)]
d[is.na(d)] <- 0 #para cachar los errores de underflow
d
pis<- data.frame(table(data$cluster))
pis
z
data[i,'cluster']
z !=data[i,'cluster']
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
d
d[is.na(d)] <- 0
d
(1:k)[which.min(d)]#nueva asignacion de cluster para la observacion
######### Implementacion de kernel k-means con shift basado en el paper:
#########Inderjit Dhillon, Yuqiang Guan and Brian Kulis.
#####A Unified view of Kernel k-means, Spectral Clustering and Graph Cuts.
Kernel.normal <- function(x,y,
alpha = (.05/2)**.5) #kernel gaussiano
#  alpha = 0.0013984457316839 ) #para el caso de vinos
{
return(exp(-sum((x-y)**2)/(2*alpha**2)))
}
Kernel.poli <- function(x,y,c = 1, d =2) #kernel polinomial
{
return((sum(x*y)+c)**d)
}
Kernel.kmeans.init <- function(data, cols,  f=Kernel.normal )
{
# data (data.frame): data set con las observaciones
# cols (vector numeric): indices de las columnas para el calculo
#de las distancias de 'data'
#f (function): kernel a usar
#se genera un dataframe con los pares de indices de las observaciones
#con la finalidad de agilizar el calculo de la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
n <- dim(data)[1]
indices <- 1:n
#modificacion del 31 de marzo solo calcular la diagonal superior de la matriz K
index <- data.frame(x1 = rep(indices,times=n:1))
x2 <-lapply(FUN = function(x){
return(n:x)
}, 1:n)
x2 <- unlist(x2)
index$x2 <-x2
index <- index[ order(index$x1, index$x2),]
library(parallel)#para incrementar la velocidad usaremos calculo multicore
#comienza calculo de la matriz superior del kernel entre todos los pares de observaciones
kernel.sup <- lapply(FUN=function(i, cols){
f(x=data[index[i,1],][cols],
y=data[index[i,2],][cols])
}, X = 1:dim(index)[1], cols)#,  mc.cores = detectCores()-2 )
kernel.sup <- unlist(kernel.sup)
MK <- matrix(rep(0, n*n), ncol = n)
for(i in 1:dim(index)[1])
{
x <- index[i, 1]
y <- index[i, 2]
MK[x, y] <- kernel.sup[i]
MK[y, x] <- kernel.sup[i]
}
#termina calculo de matriz de kernel
function(data, sigma,t, k )
{
#data (data.frame) con las observaciones a clasificar
#sigma (numeric): shift mencionado en el paper
#t (numeric): numero de iteraciones
#k (numeric): numero de clusters
data$cluster <- sample(1:k,dim(data)[1], replace = TRUE)#asignacion inicial aleatoria
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
(1:k)[which.min(d)]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
return(data$cluster) #cluster finales
}
}
############################################################
set.seed(0)
#####simulacion de datos parecidos a los del paper mencionado
#####son dos circunferencias con centro (.5,.5) y radios 1 y 4
#####se agrega en cada eje ruido ~ N(0,sigma=1/10 ) y N(0,1/10)
r <- 1 #radio
n <- 100 #la cuarta parte del numero de puntos que se van a generar
#se genera la primer circunferencia con ruido
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/10)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/10)
m.a1 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=1)
#se genera la segunda circunferencia con ruido
r <- 4
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/40)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/40)
m.a2 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=2)
m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
set.seed(0)
label<- kmeans(m.a, centers = 2, nstart = 100) #comparamos el desempeño de kmeans en vista
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .49%)') +xlab('') + ylab('')
p1
#visualizamos los resultados
#de clasificacion usando kmeans
sum(diag(as.matrix(table(m.a$clase, label$cluster))))/dim(m.a)[1] #se calcula el accuracy que es de .265, .735,.4975
#uso sobre
set.seed(0)
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a, cols=1:2)
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 20, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
p3
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 200, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
p3
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 100, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
p3
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 100, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
p3
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 150, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
p3
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 100, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
p3
setwd('C:\\Users\\fou-f\\Desktop\\MCE\\Second\\CienciaDeDatos\\tarea2\\data_fruits_tarea')
imagenes <- dir()
imagenes
dir()
head(imagenes)
library(imager)
L2 <- function(x,y) #funcion para medir distancia entre pixeles
{
# x (vector-numeric): pixel de la imagen
# y (vector-numeric): pixel con valores (1,1,1)
return(sqrt(sum((x-y)**2)))
}
index <- length(imagenes)
medianas <- matrix(rep(-1,3*index), nrow = index) #se reserva espacio para guardar las medianas de los incisos 1 a 3
##########inicia etapa de preprosemiento la cual consiste en trabajar
##########solo con pixeles que difieren en una pequena distancia del pixel blanco
for(i in 1:index)
{
fruta <- load.image(file=imagenes[i])
print(imagenes[i])
copia <- fruta
# for(j in 1:100)#para cada pixel se mide su distancia con el pixel blanco
# {
#   for(k in 1:100)
#   {
#     if(  L2(copia[j,k,,1:3], rep(1.,3)) <=0.02037707)
#     {
#       #aproximadamente si el pixel dista en menos de 3 valores del blanco se elimina de la imagen
#       copia[j,k,1:3] <- rep(NA, 3)
#     }
#   }
# }
#plot(copia)
for(j in 1:3)
{
#se calculan las medianas requeridas solamente sobre los pixeles
#considerados como informativos de la propia imagen
mediana <- median(copia[,,j], na.rm = TRUE)
medianas[i, j] <- mediana
}
remove(copia)
remove(fruta)
}
#####se termina preprocesamiento
colnames(medianas) <- c('r', 'g', 'b')
#revise si habia diferencias en el encoding, porque la función da puntos en [0,1]
#mientras yo esperaba enteros en [0,255] y no hubo diferencias
reduce.a.medianas.discreta <- function(index)
{
fruta <- load.image(file=imagenes[index])
fruta[,,1:3] <- fruta[,,1:3]*255
medianas <- apply(fruta[,,1:3], 3, median)
medianas <- matrix(medianas, ncol = 1)
row.names(medianas) <- c('r', 'g', 'b')
return(medianas)
}
#se manipulan las medianas para convertirlas en un data.frame facil de manjer en las
#visualizaciones siguientes
imagenes.medianas <- medianas
df.imagenes.medianas <- as.data.frame(imagenes.medianas)
#no.normalizados <- lapply(1:length(imagenes), FUN= reduce.a.medianas.discreta)
#no.normalizados <- do.call("cbind", no.normalizados)
#df.no.normalizados <- as.data.frame(no.normalizados)
#df.no.normalizados <- as.data.frame(t(as.matrix(df.no.normalizados)))
#se obtienen los nombres de las imagenes provenientes del directorio
library(stringr)
imagenes.nombres <-str_split(imagenes,regex("[:digit:]"),simplify = T)[,1]# "Eliminar de un digito hacia adelante"
imagenes.nombres <- str_split(imagenes.nombres, regex("$"),simplify = T)[,1]  # Elimnar los "" finales
imagenes.nombres <-str_split(imagenes.nombres, regex("r$"),simplify = T)[,1]  # "Eliminar de un "_r" hacia adelante"
df.imagenes.medianas$tipo <- imagenes.nombres #se agregan al conjunto de datos los nombres 'tipos' de frutas
df.imagenes.medianas$tipo <- factor(df.imagenes.medianas$tipo)
df.imagenes.medianas$tipo2 <- df.imagenes.medianas$tipo
#en la siguiente seccion se crea una columna que agrupa por tipo de fruta sin importar su orientacion
levels(df.imagenes.medianas$tipo2)  <- c(rep('Apple_Braeburn',2),
'Apple_Golden', rep('Apple_Granny_Smith', 2),
rep('Apricot', 2), rep('Avocado', 2), rep('Carambula', 2),
rep('Cherry', 2), rep('Huckleberry', 2), rep('Kiwi', 2), rep('Orange', 2),
rep('Peach', 2), rep('Pineapple', 2), rep('Strawberry', 2))
#se definen colores para las visualizaciones de los conjuntos
colores <- c('red', 'green', 'green4', 'pink4',
'black', 'yellow', 'purple', 'navy',
'brown', 'orange', 'salmon',
'darkgoldenrod1', 'magenta')
#df.no.normalizados$tipo <- df.imagenes.medianas$tipo
#df.no.normalizados$tipo2 <- df.imagenes.medianas$tipo2
library(plotly) #visualizacion en 3D de las imagenes representadas por las medianas en cada canal
#se realiza PCA sobre las medianas PCA con matriz de covarianzas
PCA <- princomp(df.imagenes.medianas[,1:3], cor = FALSE, scores = TRUE)
valores.pro <- PCA$sdev**2
cumsum(valores.pro)/sum(valores.pro) #las dos primeras componentes representan 96.73% de la varianza total
