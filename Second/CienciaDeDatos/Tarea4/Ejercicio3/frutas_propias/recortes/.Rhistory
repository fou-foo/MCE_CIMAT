m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
label<- kmeans(m.a, centers = 2) #comparamos el desempeño de kmeans en vista
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .49%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase, label$cluster))))/dim(m.a)[1] #se calcula el accuracy que es de .265, .735,.4975
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a, cols=1:2)
labels <- Kernel.kmeans.simu(data= m.a, sigma = -1, t = 20, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
sum(diag(as.matrix(table(m.a$clase,m.a$cluster))))/dim(m.a)[1]
library(kernlab)
kkmeans.m.a <- kkmeans(as.matrix(m.a[,1:2]), centers=2 ,
kernel = "rbfdot",
kpar = list(sigma = (.05/2)**.5),
alg="kkmeans")#mismo kernel y parametro del kernel
sum(diag(as.matrix(table(m.a$clase, kkmeans.m.a@.Data))))/dim(m.a)[1]#accuracy de 1
p2 <- ggplot(m.a, aes(x=x, y=y, color = factor(kkmeans.m.a@.Data)))+
geom_point() +theme_minimal()+theme(legend.position='none') +
ggtitle('Agrupamiento perfecto de kkmeans')+xlab('')+ylab('')
library(ggpubr)
ggarrange(p1,p2,p3, nrow = 3)
setwd('/home/fou/Desktop')
vinos <- read.csv('wine_quality.csv')
set.seed(0)
muestra.vinos <- sample(1:dim(vinos)[1], 650)
vinos <- vinos[muestra.vinos,]
vinos$cluster <- kmeans(vinos[,2:12], centers=10)$cluster
confusion <- as.matrix(table(vinos$quality, vinos$cluster))
sum(diag(confusion))/dim(vinos)[1]#.09
pca.vinos <- princomp(vinos[,2:12], cor = TRUE)
pca.vinos.prin <- pca.vinos$scores[,1:2]
pca.vinos.prin <- as.data.frame(pca.vinos.prin)
p1 <- ggplot(pca.vinos.prin,#visualizamos nuestro primer conjunto de prueba
aes(x=Comp.1, y=Comp.2, color = factor(vinos$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .12%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
t1 <- Sys.time()
Kernel.kmeans.vinos <- Kernel.kmeans.init(vinos, cols=2:12)
t1 <- Sys.time()-t1
t1
labels <- Kernel.kmeans.vinos(data=vinos, sigma = -1, t=20, k=10)
vinos$cluster <- labels
sum(diag(as.matrix(table(vinos$quality,vinos$cluster))))/dim(vinos)[1] #acurracy con shiff.03
ggplot(pca.vinos.prin,#visualizamos nuestro primer conjunto de prueba
aes(x=Comp.1, y=Comp.2, color = factor(vinos$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .12%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
p3 <- ggplot(pca.vinos.prin,#visualizamos nuestro primer conjunto de prueba
aes(x=Comp.1, y=Comp.2, color = factor(vinos$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de Kernel.kmeans (accuracy .16%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
kkmeans.vinos <- kkmeans(x=as.matrix(vinos[,2:12]), centers=10 ,
kernel = "rbfdot",
alg="kkmeans")
sum(diag(as.matrix(table(vinos$quality, kkmeans.vinos@.Data))))/dim(vinos)[1]#.04603 con el sigma que yo doy .1078, .1237 con el default que vale 0.00153861608749537
ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Primeras dos componentes principales')
ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Agrupamiento de kkmeans (accuracy .12%)')
p2 <- ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Agrupamiento de kkmeans (accuracy .12%)')
ggarrange(p1,p2,p3, nrow = 3)
ggarrange(p1,p2,p3, nrow = 1)
ggarrange(p1,p2,p3, ncol= 3)
p2 <- ggplot(pca.vinos.prin, aes(x=Comp.1, y=Comp.2,
color = factor(kkmeans.vinos@.Data)))+
geom_point()+theme_minimal()+ theme(legend.position='none') +
ggtitle('Agrupamiento de kkmeans (accuracy .12%)') +
xlab('Primer componente principal') + ylab('Segunda componente principal')
ggarrange(p1,p2,p3, ncol= 3)
##############################################################
library(rpart)
mypred <- function(r1,...)
{
#funcion para predecir recibe un objeto de la clase 'rpart'
#regresa la prediction maximo verosimil
y_hat <- predict(r1, ...)
y_hat <- apply(y_hat, 1, which.max)
y_hat
}
boost <- function(x, y, M, maxdepth=2, normalpha=F, cp = 0.01, minsplit = 50)
{
#funcion de boost para arboles
#x (dataframe)
#y (vector) : corresponde a las etiquetas de las observaciones de 'x'
N <- length(y)
w <- rep(1/N, N) # apriori uniforme
yhat <- matrix(0, ncol = M, nrow = N)
err <-  rep(1, M)
alpha <- rep(1/M, M) #pesos identicos al inicio
treelist <- vector('list', M) #lista para cuardar los clasificadores debiles
data.train <- data.frame(x=x,y=y)
k <- length(unique(data.train$y)) #numero de categorias
for(m in 1:M)
{
if(m%%50==0)
print(m) #para darnos una idea de en que iteracion va
treelist[[m]] <- rpart(y~., data=data.train, method='class',
control = rpart.control(cp = cp, minsplit=minsplit, maxdepth=maxdepth),
weights=w) #entremos las funciones base
yhat[,m] <- mypred(treelist[[m]]) #recalibramos
diferentes <- yhat[,m]!= as.numeric(y)
if(sum(diferentes) == 0) #por si en un conjunto facíl se logra el cero (como en el conjunto de datos 'iris')
{
indices <- lapply(treelist, FUN = function(x) {return(!is.null(x))})
indices <- unlist(indices)
cat('Error cero alcanzado en iteracion: ', m)
return(list(treelist=treelist[indices],
alpha=alpha[indices],
weights=w[indices], error=err[indices]))
}
err[m] <- sum(w*( diferentes )) /sum(w) #numero de errores en este clasificador debil 'm'
alpha[m] <- log((1-err[m])/err[m]) + log( k -1 )  #la GRAN diferencia entre arboles y adaboost
w <- w*exp(alpha[m]* ( diferentes )) #actualizacion de pesos
w <- w/sum(w**2)**.5
}
#if(normalpha)
#   alpha <- alpha/sum(alpha)
result <- list(treelist=treelist,alpha=alpha,weights=w,error=err)
class(result) <- 'boost'
return(invisible(result))
}
predict.boost <- function(bo, x, y, M=length(bo$treelist), verbose=F)
{
newdata <- data.frame(x=x,y=y)
yhat <- rep(-1,nrow(newdata))
k <- length(unique(y))
eval <- rep(-1, k)
cuenta <- 0
votos <- rep(-1,M)
for(j in 1:nrow(newdata)) #para cada observacion estimamos su prediccion
{
if(j%%100==0)
print(j) #para darnos una idea de en que iteracion va
eval <- rep(-1, k)
for(m in 1:M)#evaluamos la observacion en cada arbol
{
votos[m] <- mypred( bo$treelist[[m]], newdata=newdata[j,], weights=bo$weights )
}
for (l in 1:k) #guardamos los resultados de cada clase
{
eval[l] <- sum(bo$alpha*(votos == l))
}
yhat[j] <- which.max(eval) #regresamos la maximo verosimil
}
return(list( class=yhat))
}
##############################################################
library(rpart)
mypred <- function(r1,...)
{
#funcion para predecir recibe un objeto de la clase 'rpart'
#regresa la prediction maximo verosimil
y_hat <- predict(r1, ...)
y_hat <- apply(y_hat, 1, which.max)
y_hat
}
boost <- function(x, y, M, maxdepth=2, normalpha=F, cp = 0.01, minsplit = 50)
{
#funcion de boost para arboles
#x (dataframe)
#y (vector) : corresponde a las etiquetas de las observaciones de 'x'
N <- length(y)
w <- rep(1/N, N) # apriori uniforme
yhat <- matrix(0, ncol = M, nrow = N)
err <-  rep(1, M)
alpha <- rep(1/M, M) #pesos identicos al inicio
treelist <- vector('list', M) #lista para cuardar los clasificadores debiles
data.train <- data.frame(x=x,y=y)
k <- length(unique(data.train$y)) #numero de categorias
for(m in 1:M)
{
if(m%%50==0)
print(m) #para darnos una idea de en que iteracion va
treelist[[m]] <- rpart(y~., data=data.train, method='class',
control = rpart.control(cp = cp, minsplit=minsplit, maxdepth=maxdepth),
weights=w) #entremos las funciones base
yhat[,m] <- mypred(treelist[[m]]) #recalibramos
diferentes <- yhat[,m]!= as.numeric(y)
if(sum(diferentes) == 0) #por si en un conjunto facíl se logra el cero (como en el conjunto de datos 'iris')
{
indices <- lapply(treelist, FUN = function(x) {return(!is.null(x))})
indices <- unlist(indices)
cat('Error cero alcanzado en iteracion: ', m)
return(list(treelist=treelist[indices],
alpha=alpha[indices],
weights=w[indices], error=err[indices]))
}
err[m] <- sum(w*( diferentes )) /sum(w) #numero de errores en este clasificador debil 'm'
alpha[m] <- log((1-err[m])/err[m]) + log( k -1 )  #la GRAN diferencia entre arboles y adaboost
w <- w*exp(alpha[m]* ( diferentes )) #actualizacion de pesos
w <- w/sum(w**2)**.5
}
#if(normalpha)
#   alpha <- alpha/sum(alpha)
result <- list(treelist=treelist,alpha=alpha,weights=w,error=err)
class(result) <- 'boost'
return(invisible(result))
}
predict.boost <- function(bo, x, y, M=length(bo$treelist), verbose=F)
{
newdata <- data.frame(x=x,y=y)
yhat <- rep(-1,nrow(newdata))
k <- length(unique(y))
eval <- rep(-1, k)
cuenta <- 0
votos <- rep(-1,M)
for(j in 1:nrow(newdata)) #para cada observacion estimamos su prediccion
{
if(j%%100==0)
print(j) #para darnos una idea de en que iteracion va
eval <- rep(-1, k)
for(m in 1:M)#evaluamos la observacion en cada arbol
{
votos[m] <- mypred( bo$treelist[[m]], newdata=newdata[j,], weights=bo$weights )
}
for (l in 1:k) #guardamos los resultados de cada clase
{
eval[l] <- sum(bo$alpha*(votos == l))
}
yhat[j] <- which.max(eval) #regresamos la maximo verosimil
}
return(list( class=yhat))
}
############################################################
###################ejemplo rtificial  ###########################
n <- 100
u <- runif(300, 0, 1)
v1 <- max(6 - abs(1:100 - 11), 0)
v2 <- v1*(1:100-4)
v3 <- v1*(1:100+4)
x1 <- u[1:100]*v1+(1-u[1:100])*v2+rnorm(n)
x1 <- data.frame(x =x1, class='1')
x2 <- u[101:200]*v1 + (1-u[101:200])*v3+rnorm(n)
x2 <- data.frame(x=x2, class = '2')
x3 <- u[201:300]*v2+(1-u[201:300])*v3+rnorm(n)
x3 <- data.frame(x = x3, class = '3')
train <- rbind(x1, x2, x3)
y.train <- train$class
train$class <- NULL
set.seed(0)
n <- 1666
u <- runif(n*3, 0, 1)
v1 <- max(6 - abs(1:n - 11), 0)
v2 <- v1*(1:n-4)
v3 <- v1*(1:n+4)
x1 <- u[1:n]*v1+(1-u[1:n])*v2+rnorm(n)
x1 <- data.frame(x =x1, class='1')
x2 <- u[(n+1):2*n]*v1 + (1-u[(n+1):(2*n)])*v3+rnorm(n)
x2 <- data.frame(x=x2, class = '2')
x3 <- u[(2*n+1):(3*n)]*v2+(1-u[(2*n+1):(3*n)])*v3+rnorm(n)
x3 <- data.frame(x = x3, class = '3')
test <- rbind(x1, x2, x3)
y.test <- test$class
test$class <- NULL
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 1, cp = -1, minsplit = 50)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 1, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 1, cp = -1, minsplit = 200)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 1, cp = -1, minsplit = 300)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 2, cp = -1, minsplit = 300)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 2, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 3, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 4, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 4, cp = -1, minsplit = 500)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 4, cp = -1, minsplit = 50)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 4, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 4, cp = -1, minsplit = 50)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 7, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 7, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 12, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 12, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 12, cp = -1, minsplit = 30)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 12, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 15, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
y_hat <- predict.boost(bo = adaboost, x = test, y = y.test)
det(t(as.matrix(train))%*%(as.matrix(train)))
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 40, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 200)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 2)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 4)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 4, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 10, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 5, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 2, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 2, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 10, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 10, cp = -1, minsplit = 200)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 50, cp = -1, minsplit = 200)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 30, cp = -1, minsplit = 200)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 30, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 6000, maxdepth = 30, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 15)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 12)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 11)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 15)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 25)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
0
0
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 9)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 9)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 30)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 25)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 19)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 9)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 2)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 2)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 4)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 6)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 2)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 5)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 11, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 12, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 2)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 250, cp = -1, minsplit = 3)
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 25, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 6)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 5, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 1, cp = -1, minsplit = 20)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 1, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 100)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 50)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 25)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 15)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 10, cp = -1, minsplit = 8)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 30, cp = -1, minsplit = 10)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 20, cp = -1, minsplit = 9)
plot(adaboost$error, type = 'l')
adaboost <- boost(x = train, y = y.train , M = 600, maxdepth = 12, cp = -1, minsplit = 3)
plot(adaboost$error, type = 'l')
y_hat <- predict.boost(bo = adaboost, x = test, y = y.test)
sum(diag(table(y_hat$class, y.test)))/sum(table(y_hat$class, y.test))
y_hat <- predict.boost(bo = adaboost, x = test, y = y.test)
shiny::runApp('MCE_CIMAT/Second/CienciaDeDatos/DWD')
library("Rcpp", lib.loc="/usr/local/lib/R/site-library")
remove.packages("Rcpp", lib="/usr/local/lib/R/site-library")
remove.packages("Rcpp", lib="/usr/local/lib/R/site-library")
remove.packages("Rcpp", lib="/usr/local/lib/R/site-library")
remove.packages(Rcpp)
remove.packages(RcppArmadillo)
remove.packages(RcppEigen)
remove.packages(RSpectra)
setwd('/home/fou/Desktop/MCE_CIMAT/Second/CienciaDeDatos/Tarea4/Ejercicio3/frutas_propias/recortes')
library(RcppEigen) #libreria para codigo c++
library(RSpectra) #libreria para lanczos
library(imager) #libreria para leer imagenes
library(Matrix)
library(RcppArmadillo)
library(Rcpp) #libreria para codigo c++
remove.packages
remove.packages(Rcpp)
remove.packages(RcppArmadillo.package.skeleton())
remove.packages(RcppArmadillo)
remove.packages(RcppEigen)
remove.packages(Rcpp)
