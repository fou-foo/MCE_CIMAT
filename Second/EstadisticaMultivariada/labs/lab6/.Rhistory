for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
do realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}
)
do realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}
)
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i){
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i){
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:k)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
library(Rcpp)
sourceCpp("C:\\Users\\fou-f\\Desktop\\MCE\\Second\\CienciaDeDatos\\tarea2\\kernel.cpp")
######### Implementacion de kernel k-means con shift basado en el paper:
#########Inderjit Dhillon, Yuqiang Guan and Brian Kulis.
#####A Unified view of Kernel k-means, Spectral Clustering and Graph Cuts.
Kernel.kmeans.init <- function(data )
{
# data (data.frame): data set con las observaciones y solo columnas numericas
#se calcula la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
n <- dim(data)[1]
#comienza calculo de la matriz superior del kernel entre todos los pares de observaciones
MK <- matrix(rep(0, n*n), ncol = n)
data.matrix <- as.matrix(data)
p <- dim(data.matrix)[2]
MK <- CalculaKernel(MK, n, data.matrix, p)
#termina calculo de matriz de kernel
function(data, sigma,t, k )
{
#data (data.frame) con las observaciones a clasificar
#sigma (numeric): shift mencionado en el paper
#t (numeric): numero de iteraciones
#k (numeric): numero de clusters
data$cluster <- sample(1:k,dim(data)[1], replace = TRUE)#asignacion inicial aleatoria
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i){
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:k)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
return(data$cluster) #cluster finales
}
}
############################################################
set.seed(0)
#####simulacion de datos parecidos a los del paper mencionado
#####son dos circunferencias con centro (.5,.5) y radios 1 y 4
#####se agrega en cada eje ruido ~ N(0,sigma=1/10 ) y N(0,1/10)
r <- 1 #radio
n <- 100 #la cuarta parte del numero de puntos que se van a generar
#se genera la primer circunferencia con ruido
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/10)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/10)
m.a1 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=1)
#se genera la segunda circunferencia con ruido
r <- 4
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/40)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/40)
m.a2 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=2)
m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
m.a2 <- as.data.frame(scale(m.a[,1:2]) )
m.a[,1:2] <- m.a2
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
set.seed(0)
label<- kmeans(m.a, centers = 2, nstart = 100) #comparamos el desempeÃ±o de kmeans en vista
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy 50%)') +xlab('') + ylab('')
#uso sobre
set.seed(0)
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a[, 1:2])
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 20, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
dim(m.a)[1]
table(m.a$clase,m.a$cluster)
99/400
p3
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 100, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
p3
table(m.a$clase,m.a$cluster)
200+61
261/400
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 1000, k =2)
colors()
?rgb
col2rgb(colors())
plot(1:10, pch=20, col =rainbow(n=10))
rainbow(n=10)
rainbow(n=16)
11-5
6*130
6*130*1.5
1170-550
130*6*1.5
130*6*1.5-550
2*130
citation("RcppArmadillo")
library(Rcpp) #libreria para codigo c++
library(RcppEigen) #libreria para codigo c++
library(RSpectra) #libreria para lanczos
library(imager) #libreria para leer imagenes
?attach
citation("RcppArmadillo")
setwd('C:\\Users\\fou-f\\Desktop\\MCE\\Second\\EstadisticaMultivariada\\labs\\lab6')
library("MASS")
library("alr3")
library("stats")
library("scatterplot3d")
library("psych")
library("datasets")
library("MVA")
# Practica . Aplicacion de analisis de factores a un conjunto de datos con el fin de reducir la dimensionalidad de los datos
library("MASS")
library("alr3")
install.packages("alr3")
library("alr3")
library("stats")
library("scatterplot3d")
library("psych")
library("datasets")
library("MVA")
install.packages("MVA")
ventas_coche <- read.csv("car_sales.csv", header = TRUE, sep = ",", quote = "\"",dec = ".", fill = TRUE, na.strings = "NA")
View(ventas_coche)
ventas <- na.omit(ventas_coche)
#nombre de variables
lista <- names(ventas_coche)
#resumen de la estructura de los datos
str(ventas_coche)
# Se eliminan las filas con NA utilizando la funci?n na.omit()
ventas_coche <- na.omit(ventas_coche)
#se extraen los nombres de los casos
nombres_coches <- ventas_coche[,"manufact"]
nombres_coches
# se extraen solo las variables de interes
ventas_coche1 <- ventas_coche[,5:14]
# Se revisa la correlaci?n entre las variables para ver si vale la pena realizar analisis de factores
ventas_coche1_corr <- cor(ventas_coche1)
det_vc <- det(ventas_coche1_corr)
(det_vc <- det(ventas_coche1_corr))
image(ventas_coche1_corr)
View(ventas_coche1_corr)
(det_vc <- det(ventas_coche1_corr))
#prueba de esfericidad de bartlett en R para probar la hipotesis nula de que las variables no estan correlacionadas.
# La idea es rechazar la hipotesis nula para proseguir con un analisis de factores
cortest.bartlett(ventas_coche1)
View(ventas_coche1)
#prueba de esfericidad de bartlett en R para probar la hipotesis nula de que las variables no estan correlacionadas.
# La idea es rechazar la hipotesis nula para proseguir con un analisis de factores
?cortest.bartlett(ventas_coche1)
#prueba de esfericidad de bartlett en R para probar la hipotesis nula de que las variables no estan correlacionadas.
# La idea es rechazar la hipotesis nula para proseguir con un analisis de factores
cortest.bartlett(ventas_coche1_corr, n = dim(ventas_coche)[1])
venta_coche.fa1  <- factanal(ventas_coche1,factors=1)
?fcatanal
?factanal
(venta_coche.fa1  <- factanal(ventas_coche1,factors=1))
(venta_coche.fa1p <- factanal(covmat=ventas_coche1_corr,factors=1))
#se prueba la solucion con dos factores (m=2)
(venta_coche.fa2 <- factanal(ventas_coche1, factors=2))
#se prueba la solucion con tres factores (m=3)
(venta_coche.fa3 <- factanal(ventas_coche1,factors=3))
#se prueba la solucion con cuatro factores (m=4)
(venta_coche.fa4 <- factanal(ventas_coche1,factors=4))
#se prueba la solucion con cinco factores (m=5)
(venta_coche.fa5 <- factanal(ventas_coche1,factors=5))
#se prueba la solucion con seis factores (m=6)
(venta_coche.fa6 <- factanal(ventas_coche1,factors=6))
#se prueba la solucion con cinco factores (m=5)
(venta_coche.fa5 <- factanal(ventas_coche1,factors=5))
#se prueba la solucion con seis factores (m=6)
(venta_coche.fa6 <- factanal(ventas_coche1,factors=6))
#se prueba la solucion con tres factores (m=3)
(venta_coche.fa3 <- factanal(ventas_coche1,factors=3))
#Primero se obtiene la estimacion de la matriz de correlaciones
pred3_vc <- venta_coche.fa3$loadings%*%t(venta_coche.fa3$loadings)+diag(venta_coche.fa3$uniquenesses)
round(ventas_coche1_corr-pred3_vc,digits=3)
venta_coche.fa3$loadings
#Se calcula la diferencia entre las correlaciones observadas y las predichas para m=4 factores
pred4_vc <-venta_coche.fa4$loadings%*%t(venta_coche.fa4$loadings)+diag(venta_coche.fa4$uniquenesses)
round(ventas_coche1_corr-pred4_vc,digits=3)
#Se calcula la diferencia entre las correlaciones observadas y las predichas para m=5 factores
pred5_vc <-venta_coche.fa5$loadings%*%t(venta_coche.fa5$loadings)+diag(venta_coche.fa5$uniquenesses)
round(ventas_coche1_corr-pred5_vc,digits=3)
#Se calcula la diferencia entre las correlaciones observadas y las predichas para m=6 factores
pred6_vc <-venta_coche.fa6$loadings%*%t(venta_coche.fa6$loadings)+diag(venta_coche.fa6$uniquenesses)
round(ventas_coche1_corr-pred6_vc,digits=3)
venta_coche.fa3 <- factanal(ventas_coche1,factors=3,scores="regression")
?factanal
venta_coche.fa3 <- factanal(ventas_coche1,factors=3,scores="regression")
(venta_coche.fa3 <- factanal(ventas_coche1,factors=3,scores="regression"))
(venta_coche.fa3 <- factanal(ventas_coche1,factors=3,scores="Bartlett"))
(venta_coche.fa3 <- factanal(ventas_coche1,factors=3,scores="regression"))
scores <- venta_coche.fa3$scores
#se a?aden los nombres de los casos en los factor scores
dimnames(scores)[[1]] <- nombres_coches
#se a?aden los nombres de los casos en los factor scores
dimnames(scores)[[1]] <- nombres_coches
#se grafican los casos de acuerdo a los factor scores
scatterplot3d(scores, angle=35, col.grid="lightblue", main="Grafica de los factor scores", pch=20)
#se genera la matriz con  los gr?ficos de los factor scores tomando dos factores a la vez
pairs(scores)
plot(scores[,1],scores[,2],
ylim=range(scores[,1]),
xlab="Factor 1",ylab="Factor 2",type="n",lwd=2)
#f1 x f2
par(pty="s")
plot(scores[,1],scores[,2],
ylim=range(scores[,1]),
xlab="Factor 1",ylab="Factor 2",type="n",lwd=2)
text(scores[,1],scores[,2],
labels=abbreviate(row.names(scores),minlength=8),cex=0.6,lwd=2)
#f1 x f3
par(pty="s")
plot(scores[,1],scores[,3],
ylim=range(scores[,1]),
xlab="Factor 1",ylab="Factor 3",type="n",lwd=2)
text(scores[,1],scores[,3],
labels=abbreviate(row.names(scores),minlength=8),cex=0.6,lwd=2)
#f2 x f3
par(pty="s")
plot(scores[,2],scores[,3],
ylim=range(scores[,2]),
xlab="Factor 2",ylab="Factor 3",type="n",lwd=2)
text(scores[,2],scores[,3],
labels=abbreviate(row.names(scores),minlength=8),cex=0.6,lwd=2)
#Segundo conjunto de datos
telcom <- read.csv("telco.csv", header = TRUE, sep = ",", quote = "\"",dec = ".", fill = TRUE, na.strings = "NA")
#nombre de variables
lista<- names(telcom)
#se identifican las variables (servicios) de interes para el analisis
var_int<- c(16, 17, 18, 19, 20, 26, 27, 28, 29, 30, 31, 32, 33, 34)
# se extraen solo las variables de interes
telcom1 <-telcom[,var_int]
# Se eliminan las filas con NA utilizando la funci?n na.omit()
telcom1 <- na.omit(telcom1)
# Se revisa la correlaci?n entre las variables para ver si vale la pena realizar analisis de factores
telcom1_corr <- cor(telcom1)
det_telcom <- det(telcom1_corr)
#prueba de esfericidad de bartlett
cortest.bartlett(telcom1)
# se realiza un analisis de factores (utilizando MV en la estimacion), probando  de 1 a 6 factores y considerando una rotacion varimax
telcom_fa <- vector ("list",6)
for(i in 1:6){
telcom_fa[[i]] <-  factanal(covmat=telcom1_corr,factors=i,n.obs=1000)
}
(telcom.fa4 <- factanal(telcom1,factors=4))
image(telcom1_corr)
(det_telcom <- det(telcom1_corr))
#prueba de esfericidad de bartlett
p <- cortest.bartlett(telcom1)
str(p)
p$p.value
telcom_fa <- c()
for(i in 1:6){
telcom_fa[[i]] <-  factanal(covmat=telcom1_corr,factors=i,n.obs=1000)
}
(telcom.fa4 <- factanal(telcom1,factors=4))
#Se calcula la diferencia entre las correlaciones observadas y las predichas para m=4 factores
pred4_telcom <-telcom.fa4$loadings%*%t(telcom.fa4$loadings)+diag(telcom.fa4$uniquenesses)
round(telcom1_corr-pred4_telcom,digits=3)
telcom.fa4$loadings
round(telcom1_corr-pred4_telcom,digits=3)
(telcom.fa4 <- factanal(telcom1,factors=4))
View(telcom)
## Conjuntos de datos 3
## Marcadores gen?ticos
hemangioma <- read.table("hemangioma.txt",header=TRUE)
## Revision de correlaciones
cor(hemangioma)
## Revision de correlaciones
image(cor(hemangioma))
#prueba de esfericidad de bartlett
cortest.bartlett(hemangioma)
## Analisis de factores sencillo
factanal(hemangioma, factors = 3)
View(hemangioma)
det(cor(hemangioma))
det(cov(hemangioma))
12*4
3626/52
## Esperanza de vida
## str(life)
## Numero de factores
life <- read.csv("life.csv",row.names=1)
sapply(1:3, function(f) factanal(life, factors = f, method ="mle")$PVAL)
## Solucion de tres factores
factanal(life, factors = 3, method ="mle")
## Scores de los factores
scores <- factanal(life, factors = 3, method = "mle", scores = "regression")$scores
gfactorx <- 1
gfactory <- 2
par(pty="s")
plot(scores[,gfactorx],scores[,gfactory],
ylim=range(scores[,gfactory]),
xlab=paste("Factor ", gfactorx),ylab=paste("Factor", gfactory), type="n",lwd=2)
text(scores[,gfactorx],scores[,gfactory],
labels=abbreviate(row.names(scores),minlength=8),cex=0.6,lwd=2)
## Solucion de tres factores
factanal(life, factors = 3, method ="mle")
## Solucion de tres factores
p <- factanal(life, factors = 3, method ="mle")
p
## Conjunto de datos 5
## uso de drogas
druguse <- as.matrix(read.csv("druguse.csv",row.names=1))
str(druguse)
## Numero de factores
sapply(1:7, function(nf) factanal(covmat = druguse, factors = nf, method = "mle", n.obs = 1634)$PVAL)
View(druguse)
## Solucion propuesta
factanal(covmat = druguse, factors = 6,method = "mle", n.obs = 1634)
## Diferencia en las matrices calculadas y la real de covarianza
pfun <- function(nf) {
fa <- factanal(covmat = druguse, factors = nf, method = "mle", n.obs = 1634)
est <- tcrossprod(fa$loadings) + diag(fa$uniquenesses)
ret <- round(druguse - est, 3)
colnames(ret) <- rownames(ret) <- abbreviate(rownames(ret), 3)
ret
}
pfun(6)
nf <- 3
fa <- factanal(covmat = druguse, factors = nf, method = "mle", n.obs = 1634)
nf <- 6
(fa <- factanal(covmat = druguse, factors = nf, method = "mle", n.obs = 1634))
(est <- tcrossprod(fa$loadings) + diag(fa$uniquenesses))
(ret <- round(druguse - est, 3))
(colnames(ret) <- rownames(ret) <- abbreviate(rownames(ret), 3))
(fa <- factanal(covmat = druguse, factors = nf, method = "mle", n.obs = 1634))
nf <- 6
(fa <- factanal(covmat = druguse, factors = nf, method = "mle", n.obs = 1634))
(fa <- factanal(covmat = druguse, factors = nf, method = "mle", n.obs = 1634))
